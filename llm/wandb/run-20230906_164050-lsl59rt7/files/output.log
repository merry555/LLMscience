





Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 54840/54840 [00:11<00:00, 4696.14 examples/s]
Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 4105.36 examples/s]

Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:13<00:00,  6.87s/it]
You shouldn't move a model when it is dispatched on multiple devices.
Traceback (most recent call last):
  File "/home/jisukim/LLMscience/llm/finetune.py", line 144, in <module>
    train(config)
  File "/home/jisukim/LLMscience/llm/finetune.py", line 98, in train
    base_model = AutoModelForCausalLM.from_pretrained(
  File "/home/jisukim/.cache/pypoetry/virtualenvs/llmscience-qSp4PjaH-py3.8/lib/python3.8/site-packages/accelerate/big_modeling.py", line 411, in wrapper
    return fn(*args, **kwargs)
  File "/home/jisukim/.cache/pypoetry/virtualenvs/llmscience-qSp4PjaH-py3.8/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2048, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.