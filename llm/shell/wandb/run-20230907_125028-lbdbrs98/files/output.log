





Map: 100%|██████████| 54840/54840 [00:13<00:00, 4060.24 examples/s]

Map: 100%|██████████| 200/200 [00:00<00:00, 3522.02 examples/s]

Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.35s/it]
/home/jisukim/.cache/pypoetry/virtualenvs/llmscience-qSp4PjaH-py3.8/lib/python3.8/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(

Map: 100%|██████████| 54840/54840 [00:04<00:00, 11807.52 examples/s]
Map: 100%|██████████| 200/200 [00:00<00:00, 6719.06 examples/s]
/home/jisukim/.cache/pypoetry/virtualenvs/llmscience-qSp4PjaH-py3.8/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:207: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
  0%|          | 0/34275 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.







































































































































































































































































































































  1%|▏         | 500/34275 [11:02<13:17:32,  1.42s/it]
  8%|▊         | 2/25 [00:00<00:07,  2.96it/s]








 88%|████████▊ | 22/25 [00:14<00:02,  1.32it/s]


































































































































































































































































































































  3%|▎         | 999/34275 [22:18<10:41:26,  1.16s/it]
  3%|▎         | 1000/34275 [22:19<10:11:49,  1.10s/it]








 88%|████████▊ | 22/25 [00:14<00:02,  1.29it/s]



































































































































































































































































































































  4%|▍         | 1500/34275 [33:29<12:23:26,  1.36s/it]
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.22}








 92%|█████████▏| 23/25 [00:15<00:01,  1.36it/s]





























































































































































































