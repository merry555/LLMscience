






Map: 100%|██████████| 54840/54840 [00:13<00:00, 3999.24 examples/s]
Map: 100%|██████████| 200/200 [00:00<00:00, 3508.16 examples/s]

Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.35s/it]
/home/jisukim/.cache/pypoetry/virtualenvs/llmscience-qSp4PjaH-py3.8/lib/python3.8/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(

Map: 100%|██████████| 54840/54840 [00:04<00:00, 11300.65 examples/s]
Map: 100%|██████████| 200/200 [00:00<00:00, 6778.29 examples/s]
/home/jisukim/.cache/pypoetry/virtualenvs/llmscience-qSp4PjaH-py3.8/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:207: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
  0%|          | 0/34275 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...














































































































































































































































































































































































































  1%|▏         | 500/34275 [13:51<14:57:04,  1.59s/it]
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 0.07}










 92%|█████████▏| 23/25 [00:21<00:02,  1.00s/it]






















































































































































































































































































































































































































  3%|▎         | 999/34275 [28:22<16:18:05,  1.76s/it]
  3%|▎         | 1000/34275 [28:23<15:22:16,  1.66s/it]











100%|██████████| 25/25 [00:22<00:00,  1.06it/s]













































































































































































































































































































































































































  4%|▍         | 1500/34275 [42:39<14:35:32,  1.60s/it]
  0%|          | 0/25 [00:00<?, ?it/s]











 96%|█████████▌| 24/25 [00:21<00:00,  1.04it/s]


















































































































































































































