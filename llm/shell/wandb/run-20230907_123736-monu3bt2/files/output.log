




Map: 100%|██████████| 54840/54840 [00:11<00:00, 4809.90 examples/s]
Map: 100%|██████████| 200/200 [00:00<00:00, 4127.36 examples/s]


Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.45s/it]
/home/jisukim/.cache/pypoetry/virtualenvs/llmscience-qSp4PjaH-py3.8/lib/python3.8/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(

Map: 100%|██████████| 54840/54840 [00:04<00:00, 11865.47 examples/s]
Map: 100%|██████████| 200/200 [00:00<00:00, 6690.71 examples/s]
/home/jisukim/.cache/pypoetry/virtualenvs/llmscience-qSp4PjaH-py3.8/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:207: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
  0%|          | 0/34275 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...






























































































































































































